{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac075f0",
   "metadata": {},
   "source": [
    "# 04 – Win and Playoff Modeling\n",
    "\n",
    "Model how pace and efficiency relate to win percentage and playoff probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adfd72",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Load team-season aggregates and engineer targets for win percentage and playoff odds.\n",
    "- Train baseline models (logistic and linear) using pace/efficiency/shot profile features.\n",
    "- Check feature importances and calibration by era for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if PROJECT_ROOT.name == 'notebooks':\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.pipeline.season_summary import generate_team_season_summary\n",
    "from src.data_ingest import NBADataIngestor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regular-season team summaries (use cached CSV when available).\n",
    "summary_path = Path(\"data/processed/team_season_regular.csv\")\n",
    "if summary_path.exists():\n",
    "    summary = pd.read_csv(summary_path)\n",
    "else:\n",
    "    ingestor = NBADataIngestor()\n",
    "    summary = generate_team_season_summary(ingestor, regular_season_only=True, save=False)\n",
    "\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning and feature selection.\n",
    "feature_cols = [\n",
    "    \"PACE\",\n",
    "    \"OFF_EFF_PER_100\",\n",
    "    \"DEF_EFF_PER_100\",\n",
    "    \"THREE_POINT_RATE\",\n",
    "    \"AST_TOV_RATIO\",\n",
    "]\n",
    "summary = summary.dropna(subset=feature_cols + [\"WIN_PCT\"])\n",
    "\n",
    "# Binary target: top quartile win percentage as proxy for playoff-level success.\n",
    "win_pct_threshold = summary[\"WIN_PCT\"].quantile(0.75)\n",
    "summary[\"TOP_QUARTILE_WIN\"] = (summary[\"WIN_PCT\"] >= win_pct_threshold).astype(int)\n",
    "\n",
    "X = summary[feature_cols]\n",
    "y_class = summary[\"TOP_QUARTILE_WIN\"]\n",
    "y_reg = summary[\"WIN_PCT\"]\n",
    "\n",
    "summary[[\"WIN_PCT\", \"TOP_QUARTILE_WIN\"] + feature_cols].describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification: logistic regression on standardized features.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42, stratify=y_class)\n",
    "\n",
    "log_clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=500, penalty=\"l2\", solver=\"lbfgs\"),\n",
    ")\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "probs = log_clf.predict_proba(X_test)[:, 1]\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, probs))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Extract feature weights for interpretation.\n",
    "coef = log_clf.named_steps[\"logisticregression\"].coef_.ravel()\n",
    "importance = pd.Series(coef, index=feature_cols).sort_values()\n",
    "importance.plot(kind=\"barh\", title=\"Logistic Regression Coefficients\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: predict win percentage directly.\n",
    "reg_model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "reg_model.fit(X, y_reg)\n",
    "pred_win_pct = reg_model.predict(X)\n",
    "\n",
    "rmse = mean_squared_error(y_reg, pred_win_pct, squared=False)\n",
    "print(f\"RMSE on full sample: {rmse:.4f}\")\n",
    "\n",
    "residuals = y_reg - pred_win_pct\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(residuals, kde=True, ax=ax[0])\n",
    "ax[0].set_title(\"Residual distribution\")\n",
    "sns.scatterplot(x=pred_win_pct, y=residuals, ax=ax[1])\n",
    "ax[1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "ax[1].set_xlabel(\"Predicted WIN_PCT\")\n",
    "ax[1].set_ylabel(\"Residual\")\n",
    "ax[1].set_title(\"Residuals vs Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3cca9",
   "metadata": {},
   "source": [
    "### Model zoo and report-ready metrics\n",
    "Compare multiple classifiers on the top-quartile playoff proxy and capture headline metrics for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification model zoo.\n",
    "class_models = {\n",
    "    \"LogReg\": make_pipeline(StandardScaler(), LogisticRegression(max_iter=600, penalty=\"l2\", solver=\"lbfgs\")),\n",
    "    \"SVC-linear\": make_pipeline(StandardScaler(), SVC(kernel=\"linear\", probability=True, random_state=42)),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_leaf=2, random_state=42),\n",
    "    \"GradBoost\": GradientBoostingClassifier(random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=6, random_state=42),\n",
    "    \"KNN\": make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=15)),\n",
    "}\n",
    "\n",
    "class_records = []\n",
    "for name, model in class_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "    preds = (probas >= 0.5).astype(int)\n",
    "    class_records.append(\n",
    "        {\n",
    "            \"model\": name,\n",
    "            \"roc_auc\": roc_auc_score(y_test, probas),\n",
    "            \"accuracy\": accuracy_score(y_test, preds),\n",
    "            \"f1\": f1_score(y_test, preds),\n",
    "        }\n",
    "    )\n",
    "\n",
    "class_df = pd.DataFrame(class_records).sort_values(\"roc_auc\", ascending=False)\n",
    "class_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b15118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the top-performing classifier.\n",
    "best_model_name = class_df.iloc[0][\"model\"]\n",
    "best_model = class_models[best_model_name]\n",
    "best_preds = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, best_preds)\n",
    "print(f\"Best classifier: {best_model_name}\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67827ba0",
   "metadata": {},
   "source": [
    "### Calibration (regular-season best classifier)\n",
    "Reliability curve and Brier score for the top model to report probability quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_best = best_model.predict_proba(X_test)[:, 1]\n",
    "brier = brier_score_loss(y_test, probs_best)\n",
    "prob_true, prob_pred = calibration_curve(y_test, probs_best, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "print(f\"Brier score (lower is better): {brier:.4f}\")\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Perfect calibration\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bff03",
   "metadata": {},
   "source": [
    "### Regression baselines\n",
    "Benchmark RMSE/MAE/R² for predicting win percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2286cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_models = {\n",
    "    \"Linear\": make_pipeline(StandardScaler(), LinearRegression()),\n",
    "    \"Ridge\": make_pipeline(StandardScaler(), Ridge(alpha=1.0)),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=400, random_state=42, min_samples_leaf=2),\n",
    "    \"GradBoost\": GradientBoostingRegressor(random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(max_depth=8, random_state=42),\n",
    "    \"KNN\": make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=15)),\n",
    "}\n",
    "\n",
    "reg_records = []\n",
    "for name, model in reg_models.items():\n",
    "    model.fit(X, y_reg)\n",
    "    preds = model.predict(X)\n",
    "    reg_records.append(\n",
    "        {\n",
    "            \"model\": name,\n",
    "            \"rmse\": mean_squared_error(y_reg, preds, squared=False),\n",
    "            \"mae\": mean_absolute_error(y_reg, preds),\n",
    "            \"r2\": r2_score(y_reg, preds),\n",
    "        }\n",
    "    )\n",
    "\n",
    "reg_df = pd.DataFrame(reg_records).sort_values(\"rmse\")\n",
    "reg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792fe6f",
   "metadata": {},
   "source": [
    "### Feature ablations\n",
    "Remove one feature at a time to quantify its contribution to classification AUC and regression RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48246e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ablation(drop_cols):\n",
    "    cols = [c for c in feature_cols if c not in drop_cols]\n",
    "    X_sub = summary[cols].dropna()\n",
    "    y_class_sub = summary.loc[X_sub.index, \"TOP_QUARTILE_WIN\"]\n",
    "    y_reg_sub = summary.loc[X_sub.index, \"WIN_PCT\"]\n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "        X_sub, y_class_sub, test_size=0.2, random_state=42, stratify=y_class_sub\n",
    "    )\n",
    "    clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=600))\n",
    "    clf.fit(X_train_sub, y_train_sub)\n",
    "    probas = clf.predict_proba(X_test_sub)[:, 1]\n",
    "    auc = roc_auc_score(y_test_sub, probas)\n",
    "\n",
    "    reg = make_pipeline(StandardScaler(), LinearRegression())\n",
    "    reg.fit(X_sub, y_reg_sub)\n",
    "    rmse = mean_squared_error(y_reg_sub, reg.predict(X_sub), squared=False)\n",
    "    return auc, rmse\n",
    "\n",
    "ablation_records = []\n",
    "for col in feature_cols:\n",
    "    auc, rmse = evaluate_ablation([col])\n",
    "    ablation_records.append({\"dropped\": col, \"auc_no_feat\": auc, \"rmse_no_feat\": rmse})\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_records).sort_values(\"auc_no_feat\", ascending=False)\n",
    "ablation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be832e23",
   "metadata": {},
   "source": [
    "These tables can drop straight into the LaTeX report (classification AUC/accuracy/F1, regression RMSE/MAE/R², and ablation deltas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc1e37",
   "metadata": {},
   "source": [
    "### Playoff-only evaluation\n",
    "Re-run the classification zoo on playoff team-seasons to capture postseason alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574aa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "playoff_path = Path(\"data/processed/team_season_playoffs.csv\")\n",
    "if playoff_path.exists():\n",
    "    playoff = pd.read_csv(playoff_path)\n",
    "else:\n",
    "    ingestor = NBADataIngestor()\n",
    "    playoff = generate_team_season_summary(ingestor, playoffs_only=True, save=False)\n",
    "\n",
    "playoff = playoff.dropna(subset=feature_cols + [\"WIN_PCT\"])\n",
    "if len(playoff) < 20:\n",
    "    print(f\"Playoff sample too small for robust eval (n={len(playoff)}).\")\n",
    "else:\n",
    "    playoff_threshold = playoff[\"WIN_PCT\"].quantile(0.75)\n",
    "    playoff[\"TOP_QUARTILE_WIN\"] = (playoff[\"WIN_PCT\"] >= playoff_threshold).astype(int)\n",
    "    X_po = playoff[feature_cols]\n",
    "    y_po = playoff[\"TOP_QUARTILE_WIN\"]\n",
    "    X_train_po, X_test_po, y_train_po, y_test_po = train_test_split(\n",
    "        X_po, y_po, test_size=0.2, random_state=42, stratify=y_po\n",
    "    )\n",
    "\n",
    "    po_records = []\n",
    "    for name, model in class_models.items():\n",
    "        model.fit(X_train_po, y_train_po)\n",
    "        probas_po = model.predict_proba(X_test_po)[:, 1]\n",
    "        preds_po = (probas_po >= 0.5).astype(int)\n",
    "        po_records.append(\n",
    "            {\n",
    "                \"model\": name,\n",
    "                \"roc_auc\": roc_auc_score(y_test_po, probas_po),\n",
    "                \"accuracy\": accuracy_score(y_test_po, preds_po),\n",
    "                \"f1\": f1_score(y_test_po, preds_po),\n",
    "            }\n",
    "        )\n",
    "    playoff_df = pd.DataFrame(po_records).sort_values(\"roc_auc\", ascending=False)\n",
    "    playoff_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd140ac",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Add era-stratified splits or time-based CV to test stability of coefficients.\n",
    "- Calibrate classifiers (Platt/Isotonic) and compare lift vs uncalibrated.\n",
    "- Add playoff-only split and compare deltas against regular-season models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
